# -*- coding: utf-8 -*-
"""ì§ˆë³‘ ì˜ˆì¸¡ ëª¨ë¸.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NdPCB1ocWlULcpMKTQjdEhLFIE0QbbTb

# ë°ì´í„° ì „ì²˜ë¦¬
"""

 # â† replace with the result of `which dot` if different


import pandas as pd

# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
try:
    # ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ ì›ë³¸ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    file_path = '/Users/jay/Desktop/Illness Prediction/Processed Data/Illness & Environmental/final_data/Acute_upper_respiratory_infections/merged_data_Acute_upper_respiratory_infections_lag0.csv'
    df = pd.read_csv(file_path)
    print(f"'{file_path}' íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
except FileNotFoundError:
    print(f"ì˜¤ë¥˜: '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì´ ì½”ë“œì™€ ë™ì¼í•œ í´ë”ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë” ì´ìƒ ì§„í–‰í•˜ì§€ ì•ŠìŒ
    exit()

# 2. ë°ì´í„° ì •ì œ ë° ê¸°ë³¸ ì¤€ë¹„
# ë¶„ì„ì— ìš©ì´í•˜ë„ë¡ ì»¬ëŸ¼ëª… ë³€ê²½
df.rename(columns={
    'ParsedDateTime': 'Date',
    'CaseCount': 'Case_Count',
    'RegionName': 'Region'
}, inplace=True)

# 'Date' ì»¬ëŸ¼ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
df['Date'] = pd.to_datetime(df['Date'])

# ì‹œê³„ì—´ ë¶„ì„ì„ ìœ„í•´ ì§€ì—­(Region)ê³¼ ë‚ ì§œ(Date) ìˆœìœ¼ë¡œ ë°ì´í„° ì •ë ¬
df.sort_values(by=['Region', 'Date'], inplace=True)

# ì¤‘ë³µëœ í–‰ ì œê±°
df.drop_duplicates(inplace=True)

# ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ë“¤ ì‚­ì œ
# 'IllnessName' ì»¬ëŸ¼ì€ ê°’ì´ í•˜ë‚˜ë¿ì´ë¯€ë¡œ ì œê±°
if 'IllnessName' in df.columns and df['IllnessName'].nunique() == 1:
    df.drop('IllnessName', axis=1, inplace=True)

# ê¸°íƒ€ ì¤‘ë³µ ì˜ë¯¸ë¥¼ ê°–ëŠ” ì»¬ëŸ¼ë“¤ ì œê±°
cols_to_drop = ['DateTime', 'Year_x', 'Year_y', 'RegionCode']
cols_to_drop_exist = [col for col in cols_to_drop if col in df.columns]
df.drop(columns=cols_to_drop_exist, inplace=True)

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ì‹œê³„ì—´ ë°ì´í„°ì´ë¯€ë¡œ ì´ì „ ê°’ìœ¼ë¡œ ì±„ìš´ í›„, ë‚¨ì€ ê²ƒì€ ë‹¤ìŒ ê°’ìœ¼ë¡œ ì±„ì›€)
df.fillna(method='ffill', inplace=True)
df.fillna(method='bfill', inplace=True)
print("ë°ì´í„° ì •ì œ ë° ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# 3. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
print("\ní”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
df_featured = df.copy()

# 3-1. ì‹œê°„ ê¸°ë°˜ í”¼ì²˜ ìƒì„±
df_featured['Year'] = df_featured['Date'].dt.year
df_featured['Month'] = df_featured['Date'].dt.month
df_featured['DayOfWeek'] = df_featured['Date'].dt.dayofweek  # (ì›”ìš”ì¼=0, ì¼ìš”ì¼=6)
df_featured['WeekOfYear'] = df_featured['Date'].dt.isocalendar().week.astype(int)
df_featured['DayOfYear'] = df_featured['Date'].dt.dayofyear
print("  - ì‹œê°„ ê¸°ë°˜ í”¼ì²˜ ìƒì„± ì™„ë£Œ (Year, Month, DayOfWeek ë“±)")

# 3-2. ì§€ì—°(Lag) í”¼ì²˜ ìƒì„±
# ê³¼ê±°ì˜ ë°ì´í„°ê°€ í˜„ì¬ì— ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì„ ê°€ì •í•˜ì—¬ ìƒì„±
target = 'Case_Count'
features_to_lag = [target, 'AvgTemp', 'PM10', 'AvgHumidity']
lag_days = [7, 14, 21]  # 7ì¼, 14ì¼, 21ì¼ ì „ ë°ì´í„°

for feature in features_to_lag:
    if feature in df_featured.columns:
        for lag in lag_days:
            col_name = f'{feature}_lag_{lag}'
            # ì§€ì—­(Region)ë³„ë¡œ ê·¸ë£¹ì„ ë‚˜ëˆ  ì§€ì—° í”¼ì²˜ ê³„ì‚°
            df_featured[col_name] = df_featured.groupby('Region')[feature].shift(lag)
print(f"  - ì§€ì—°(Lag) í”¼ì²˜ ìƒì„± ì™„ë£Œ (Lag: {lag_days}ì¼)")

# 3-3. ì´ë™ í‰ê· (Rolling Window) í”¼ì²˜ ìƒì„±
# ë°ì´í„°ì˜ ë‹¨ê¸°ì ì¸ ì¶”ì„¸ ë° ë³€ë™ì„±ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ìƒì„±
window_sizes = [7, 14] # 7ì¼, 14ì¼ ì´ë™ í‰ê· 
for feature in features_to_lag:
    if feature in df_featured.columns:
        # í˜„ì¬ ê°’ì„ í¬í•¨í•˜ì§€ ì•Šë„ë¡ shift(1) ì ìš© (Data Leakage ë°©ì§€)
        shifted_series = df_featured.groupby('Region')[feature].shift(1)
        for window in window_sizes:
            mean_col_name = f'{feature}_rolling_mean_{window}'
            std_col_name = f'{feature}_rolling_std_{window}'
            # ì§€ì—­(Region)ë³„ë¡œ ê·¸ë£¹ì„ ë‚˜ëˆ  ì´ë™ í‰ê· /í‘œì¤€í¸ì°¨ ê³„ì‚°
            df_featured[mean_col_name] = shifted_series.rolling(window=window, min_periods=1).mean()
            df_featured[std_col_name] = shifted_series.rolling(window=window, min_periods=1).std()
print(f"  - ì´ë™ í‰ê· (Rolling) í”¼ì²˜ ìƒì„± ì™„ë£Œ (Window: {window_sizes}ì¼)")

# 4. ìµœì¢… ë°ì´í„° ì¤€ë¹„
# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ê³¼ì •ì—ì„œ ìƒê¸´ ê²°ì¸¡ì¹˜ ë‹¤ì‹œ í•œë²ˆ ì²˜ë¦¬
df_featured.fillna(method='bfill', inplace=True)
df_featured.fillna(0, inplace=True) # ë§¨ ì•ë¶€ë¶„ì— ë‚¨ì€ ê²°ì¸¡ì¹˜ëŠ” 0ìœ¼ë¡œ ì±„ì›€

# ì›ë³¸ 'Date' ì»¬ëŸ¼ì€ ì´ì œ ë¶ˆí•„ìš”í•˜ë¯€ë¡œ ì œê±°
df_featured.drop('Date', axis=1, inplace=True)

# ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ëª¨ë¸ì´ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ ì›-í•« ì¸ì½”ë”© ì²˜ë¦¬
categorical_cols = ['Region', 'Season']
for col in categorical_cols:
    if col in df_featured.columns:
        df_featured = pd.get_dummies(df_featured, columns=[col], prefix=col)
print("ë²”ì£¼í˜• ë°ì´í„°(Region, Season)ë¥¼ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")


# 5. ìµœì¢… ê²°ê³¼ë¬¼ ì €ì¥
output_filename = 'feature_engineered_integrated_data.csv'
df_featured.to_csv(output_filename, index=False)

print(f"\nğŸ‰ ëª¨ë“  ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! '{output_filename}' íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("\n--- ìµœì¢… ë°ì´í„° ì •ë³´ ---")
df_featured.info()

df_featured

"""# ëª¨ë¸ ì„±ëŠ¥ í™•ì¸(XGB)"""

import pandas as pd
import xgboost as xgb
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from copy import deepcopy

# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
try:
    file_path = 'feature_engineered_integrated_data.csv'
    df = pd.read_csv(file_path)
    print(f"'{file_path}' íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
except FileNotFoundError:
    print(f"ì˜¤ë¥˜: '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ì „ ë‹¨ê³„ì˜ ì½”ë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ íŒŒì¼ì„ ìƒì„±í•´ì£¼ì„¸ìš”.")
    exit()

# 2. í”¼ì²˜(X)ì™€ íƒ€ê²Ÿ(y) ë¶„ë¦¬
X = df.drop('Case_Count', axis=1)
y = df['Case_Count']

# 3. ì‹œê³„ì—´ ë°ì´í„° ë¶„í• 
test_size = int(len(df) * 0.2)
X_train, X_test = X[:-test_size], X[-test_size:]
y_train, y_test = y[:-test_size], y[-test_size:]

print(f"\ní›ˆë ¨ ë°ì´í„° í¬ê¸°: {X_train.shape}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {X_test.shape}")

# 4. XGBoost ëª¨ë¸ ìˆ˜ë™ í›ˆë ¨ ë° ì¡°ê¸° ì¢…ë£Œ
# ê¸°ë³¸ ëª¨ë¸ ì´ˆê¸°í™”
# n_estimatorsëŠ” 1ë¡œ ì„¤ì •í•˜ê³ , warm_start=Trueë¡œ ì„¤ì •í•˜ì—¬ ë°˜ë³µë§ˆë‹¤ ë‚˜ë¬´ë¥¼ í•˜ë‚˜ì”© ì¶”ê°€í•©ë‹ˆë‹¤.
xgb_reg_manual = xgb.XGBRegressor(
    objective='reg:squarederror',
    learning_rate=0.05,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1,
    n_estimators=1  # ì‹œì‘ì€ 1ê°œë¡œ
)

# ë³€ìˆ˜ ì„¤ì •
early_stopping_rounds = 50
best_rmse = float('inf')
best_iteration = 0
no_improvement_count = 0
best_model = None

print("\nXGBoost ëª¨ë¸ ìˆ˜ë™ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

# ìµœëŒ€ 1000ë²ˆ ë°˜ë³µí•˜ë©´ì„œ ë‚˜ë¬´ ì¶”ê°€
for i in range(1000):
    # warm_start=Trueë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì´ì „ì— í•™ìŠµëœ ëª¨ë¸ ê°ì²´ë¥¼ ë‹¤ì‹œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
    # í•˜ì§€ë§Œ ëª¨ë“  ë²„ì „ì—ì„œ ì´ ê¸°ëŠ¥ì´ ì•ˆì •ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ,
    # ë§¤ë²ˆ n_estimatorsë¥¼ 1ì”© ëŠ˜ë ¤ ìƒˆë¡œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ë” ì•ˆì •ì ì…ë‹ˆë‹¤.
    xgb_reg_current = xgb.XGBRegressor(
        objective='reg:squarederror', learning_rate=0.05, max_depth=5,
        subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1,
        n_estimators=i + 1 # ë‚˜ë¬´ ê°œìˆ˜ë¥¼ 1ì”© ì¦ê°€
    )
    xgb_reg_current.fit(X_train, y_train)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ë° ì„±ëŠ¥(RMSE) ê³„ì‚°
    y_pred_current = xgb_reg_current.predict(X_test)
    current_rmse = np.sqrt(mean_squared_error(y_test, y_pred_current))

    print(f"Iteration {i+1}, RMSE: {current_rmse:.4f}")

    # ìµœê³  ì„±ëŠ¥ì¸ì§€ í™•ì¸
    if current_rmse < best_rmse:
        best_rmse = current_rmse
        best_iteration = i + 1
        best_model = deepcopy(xgb_reg_current) # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        no_improvement_count = 0
    else:
        no_improvement_count += 1

    # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ í™•ì¸
    if no_improvement_count >= early_stopping_rounds:
        print(f"\n{early_stopping_rounds}ë²ˆ ì—°ì†ìœ¼ë¡œ ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šì•„ í•™ìŠµì„ ì¡°ê¸° ì¢…ë£Œí•©ë‹ˆë‹¤.")
        print(f"ìµœê³  ì„±ëŠ¥ì€ {best_iteration}ë²ˆì§¸ ë°˜ë³µì—ì„œ RMSE: {best_rmse:.4f} ì…ë‹ˆë‹¤.")
        break

# ìµœì¢… ëª¨ë¸ì€ ìµœê³  ì„±ëŠ¥ì„ ê¸°ë¡í•œ ëª¨ë¸ë¡œ ì§€ì •
final_model = best_model
print("\nXGBoost ëª¨ë¸ í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# 5. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì‚¬ìš©)
y_pred = final_model.predict(X_test)

# 6. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("\n--- ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ---")
print(f"í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (MAE): {mae:.2f}")
print(f"í‰ê·  ì œê³± ì˜¤ì°¨ (MSE): {mse:.2f}")
print(f"í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨ (RMSE): {best_rmse:.2f}") # ì¡°ê¸° ì¢…ë£Œ ì‹œì ì˜ RMSE
print(f"ê²°ì • ê³„ìˆ˜ (R-squared, R2): {r2:.2f}")

# 7. í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™”
plt.figure(figsize=(12, 10))
plt.rcParams['axes.unicode_minus'] = False

xgb.plot_importance(final_model, max_num_features=20, height=0.8)
plt.title('top 20 feature_importance', fontsize=15)
plt.yticks(fontsize=12)
plt.tight_layout()
plt.savefig('feature_importance.png')
print("\ní”¼ì²˜ ì¤‘ìš”ë„ ê·¸ë˜í”„ê°€ 'feature_importance.png' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# 8. ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ì‹œê°í™”
plt.figure(figsize=(15, 7))
plt.plot(y_test.values, label='real value', color='blue', alpha=0.8)
plt.plot(y_pred, label='predicted value', color='red', linestyle='--')
plt.title('real Case Count compared model prediction', fontsize=16)
plt.xlabel('time (Test Set)', fontsize=12)
plt.ylabel('Case Count', fontsize=12)
plt.legend()
plt.grid(True)
plt.savefig('prediction_vs_actual.png')
print("ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ ë¹„êµ ê·¸ë˜í”„ê°€ 'prediction_vs_actual.png' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")



"""# ë³€ìˆ˜ ì„ íƒ(RFE)"""

import pandas as pd
import xgboost as xgb
import matplotlib.pyplot as plt
import numpy as np
from sklearn.feature_selection import RFECV
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score

# ì´ ì½”ë“œëŠ” ë°ì´í„°(X_train, y_train ë“±)ê°€ ì¤€ë¹„ë˜ì—ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.

print("ì¬ê·€ì  í”¼ì²˜ ì œê±°(RFECV)ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")

# 1. RFECVë¥¼ ìœ„í•œ ëª¨ë¸ê³¼ êµì°¨ê²€ì¦ ì„¤ì •
# ì‹œê³„ì—´ ë°ì´í„°ì´ë¯€ë¡œ TimeSeriesSplitì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
tscv = TimeSeriesSplit(n_splits=3)

# RFEì— ì‚¬ìš©í•  ê¸°ë³¸ XGBoost ëª¨ë¸
estimator = xgb.XGBRegressor(
    objective='reg:squarederror',
    max_depth=5,
    random_state=42,
    n_jobs=-1
)

# 2. RFECV ì‹¤í–‰
# step=5: ë§¤ ë°˜ë³µë§ˆë‹¤ 5ê°œì˜ ë³€ìˆ˜ì”© ì œê±°
# min_features_to_select: ìµœì†Œí•œìœ¼ë¡œ ë‚¨ê¸¸ ë³€ìˆ˜ì˜ ê°œìˆ˜
# cv: êµì°¨ê²€ì¦ ë°©ì‹
# scoring: ëª¨ë¸ í‰ê°€ ì§€í‘œ. ì—¬ê¸°ì„œëŠ” 'neg_root_mean_squared_error'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
selector = RFECV(
    estimator,
    step=5,
    min_features_to_select=20,
    cv=tscv,
    scoring='neg_root_mean_squared_error',
    verbose=1 # ì§„í–‰ ìƒí™© ì¶œë ¥
)

selector = selector.fit(X_train, y_train)

# 3. ê²°ê³¼ í™•ì¸
print(f"\nRFECVê°€ ìµœì ì˜ ë³€ìˆ˜ ê°œìˆ˜ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤: {selector.n_features_}")

# ì„ íƒëœ ë³€ìˆ˜ë“¤ì˜ ëª©ë¡
selected_features_rfe = X_train.columns[selector.support_].tolist()
print("\n--- RFECVë¡œ ì„ íƒëœ ë³€ìˆ˜ë“¤ ---")
print(selected_features_rfe)

# 4. ì„ íƒëœ í”¼ì²˜ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
X_train_rfe = X_train[selected_features_rfe]
X_test_rfe = X_test[selected_features_rfe]

# ì´ì „ í•™ìŠµ ì½”ë“œì—ì„œ ì°¾ì€ ìµœì ì˜ ë‚˜ë¬´ ê°œìˆ˜ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
final_model_rfe = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=100, # ì˜ˆì‹œ ê°’, ì´ì „ í•™ìŠµì—ì„œ ì°¾ì€ best_iteration ê°’ì„ ì‚¬ìš©í•˜ë©´ ë” ì¢‹ìŠµë‹ˆë‹¤.
    learning_rate=0.05,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1
)

final_model_rfe.fit(X_train_rfe, y_train)
y_pred_rfe = final_model_rfe.predict(X_test_rfe)

# ì„±ëŠ¥ í‰ê°€
rmse_rfe = np.sqrt(mean_squared_error(y_test, y_pred_rfe))
r2_rfe = r2_score(y_test, y_pred_rfe)

print("\n--- RFECV í”¼ì²˜ ì„ íƒ í›„ ëª¨ë¸ ì„±ëŠ¥ ---")
print(f"í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨ (RMSE): {rmse_rfe:.2f}")
print(f"ê²°ì • ê³„ìˆ˜ (R-squared, R2): {r2_rfe:.2f}")

import matplotlib.pyplot as plt

# Create the plot
plt.figure(figsize=(15, 7))
plt.plot(y_test.values, label='Actual Values', color='blue', alpha=0.8)
plt.plot(y_pred_rfe, label='Predicted Values (RFE)', color='red', linestyle='--')

# Add titles and labels in English
plt.title('Model Predictions vs. Actual Values (After RFECV)', fontsize=16)
plt.xlabel('Time Points (Test Set)', fontsize=12)
plt.ylabel('Case Count', fontsize=12)
plt.legend()
plt.grid(True)

# Save the figure
plt.savefig('rfe_prediction_vs_actual.png')
print("Actual vs. Predicted plot has been saved as 'rfe_prediction_vs_actual.png'.")


import xgboost as xgb
import matplotlib.pyplot as plt

# This code assumes you have the 'final_model_rfe' from the previous step.

# Set the figure size
plt.figure(figsize=(12, 10))

# Plot the feature importances
# The model object 'final_model_rfe' should be available from your previous run.
xgb.plot_importance(final_model_rfe, max_num_features=40, height=0.8)

# --- MODIFIED PART ---
# Adjust font sizes for the title and axis labels
plt.title('Feature Importance (After RFECV)', fontsize=14) # Title font size
plt.xlabel('F-Score', fontsize=10)
plt.ylabel('Features', fontsize=10)
plt.yticks(fontsize=8) # <--- Reduced font size for feature names

plt.tight_layout()

# Save the figure
plt.savefig('rfe_feature_importance_small_font.png')
print("Feature importance plot with smaller font has been saved as 'rfe_feature_importance_small_font.png'.")

# This code should run after the selector.fit(X_train, y_train) command.

# 3. ê²°ê³¼ í™•ì¸ (Formatted Output)
print(f"\nRFECV has found the optimal number of features: {selector.n_features_}")

# Get the list of selected feature names
selected_features_rfe = X_train.columns[selector.support_].tolist()

print("\n--- Features Selected by RFECV ---")

# Loop through the list and print each feature with its number
for i, feature in enumerate(selected_features_rfe):
    print(f"{i+1:2d}. {feature}") # Prints a numbered list

"""# ì„ íƒëœ ë³€ìˆ˜ë¥¼ í†µí•´ íŒŒë¼ë¯¸í„° íŠœë‹ (70/15/15)"""

import pandas as pd
import xgboost as xgb
import matplotlib.pyplot as plt
import numpy as np
import optuna
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error
from copy import deepcopy

# --- 1. Load and Prepare Data ---

try:
    file_path = 'feature_engineered_integrated_data.csv'
    df = pd.read_csv(file_path)
    print(f"Successfully loaded '{file_path}'.")
except FileNotFoundError:
    print(f"Error: Could not find '{file_path}'. Please run the data processing script first.")
    exit()

# List of 87 features selected by your RFECV run.
selected_features_rfe = [
    'Month', 'AvgTemp', 'MinTemp', 'MaxTemp', 'Rainfall', 'CloudCover',
    'Max10minRain', 'Max1hrRain', 'RainfallHours', 'Rain9to9', 'MaxWindSpeed',
    'MaxWindDir', 'AvgWindSpeed', 'TotalWindCalm', 'MinHumidity', 'AvgHumidity',
    'AvgVaporPressure', 'AvgDewPoint', 'AvgLocalPressure', 'AvgSeaLevelPressure',
    'MaxSeaLevelPressure', 'MinSeaLevelPressure', 'SunshineHours',
    'SolarRadiationHours', 'Max1hrSolarRadiation', 'DailySolarRadiation',
    'MaxSnowDepth', 'MaxNewSnowDepth', 'NewSnow3hr', 'MidLowCloudCover',
    'AvgGroundTemp', 'MinGrassTemp', 'AvgSoilTemp5cm', 'AvgSoilTemp10cm',
    'AvgSoilTemp20cm', 'AvgSoilTemp30cm', 'SoilTemp0_5m', 'SoilTemp1_0m',
    'SoilTemp1_5m', 'SoilTemp3_0m', 'SoilTemp5_0m', 'TotalLargeEvaporation',
    'TotalSmallEvaporation', 'FogDuration', 'SO2', 'CO', 'O3', 'NO2', 'PM10',
    'PM25', 'Year', 'DayOfWeek', 'WeekOfYear', 'DayOfYear', 'Case_Count_lag_7',
    'Case_Count_lag_14', 'Case_Count_lag_21', 'AvgTemp_lag_7', 'AvgTemp_lag_14',
    'AvgTemp_lag_21', 'PM10_lag_7', 'PM10_lag_14', 'PM10_lag_21',
    'AvgHumidity_lag_7', 'AvgHumidity_lag_14', 'AvgHumidity_lag_21',
    'Case_Count_rolling_mean_7', 'Case_Count_rolling_std_7',
    'Case_Count_rolling_mean_14', 'Case_Count_rolling_std_14',
    'AvgTemp_rolling_mean_7', 'AvgTemp_rolling_std_7',
    'AvgTemp_rolling_mean_14', 'AvgTemp_rolling_std_14',
    'PM10_rolling_mean_7', 'PM10_rolling_std_7', 'PM10_rolling_mean_14',
    'PM10_rolling_std_14', 'AvgHumidity_rolling_mean_7',
    'AvgHumidity_rolling_std_7', 'AvgHumidity_rolling_mean_14',
    'AvgHumidity_rolling_std_14', 'Region_Busan', 'Region_Daegu',
    'Region_Gyeonggi', 'Region_Incheon', 'Region_Jeju'
]

# Prepare the data with the selected features
X = df[selected_features_rfe]
y = df['Case_Count']

# --- 2. Create Train, Validation, and Test Sets ---
# Split: 70% Train, 15% Validation, 15% Test
train_size = int(len(df) * 0.70)
val_size = int(len(df) * 0.15)

X_train, y_train = X[:train_size], y[:train_size]
X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]
X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]

print("\nData Splitting:")
print(f"Training Set Shape:   {X_train.shape}")
print(f"Validation Set Shape: {X_val.shape}")
print(f"Test Set Shape:       {X_test.shape}")

# --- 3. Bayesian Optimization using Optuna ---
# This objective function is compatible with older XGBoost versions by not using early stopping inside the function.
def objective(trial):
    """Define the objective function for Optuna to optimize."""
    params = {
        'objective': 'reg:squarederror',
        'n_estimators': trial.suggest_int('n_estimators', 100, 1500), # Optuna will find the best number of trees
        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True), # L1 regularization
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True), # L2 regularization
        'n_jobs': -1,
        'random_state': 42
    }

    model = xgb.XGBRegressor(**params)

    # Train the model WITHOUT early stopping arguments for compatibility
    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)

    # Evaluate on the validation set
    preds = model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val, preds))

    return rmse

print("\nStarting Bayesian Optimization...")
# Increase n_trials for a more exhaustive search if you have time. n_jobs=1 for better compatibility.
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50, n_jobs=1)

print("Optimization finished.")
print(f"Best trial RMSE on validation set: {study.best_value:.4f}")
print("Best hyperparameters found:")
print(study.best_params)

# --- 4. Train Final Model and Evaluate ---
best_params = study.best_params
X_train_val = pd.concat([X_train, X_val])
y_train_val = pd.concat([y_train, y_val])

final_tuned_model = xgb.XGBRegressor(**best_params, n_jobs=-1, random_state=42)
final_tuned_model.fit(X_train_val, y_train_val)
y_pred_final = final_tuned_model.predict(X_test)

# Evaluate using specified metrics
r2 = r2_score(y_test, y_pred_final)
mse = mean_squared_error(y_test, y_pred_final)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred_final)
mape = mean_absolute_percentage_error(y_test, y_pred_final)

print("\n--- Final Tuned Model Evaluation on Test Set ---")
print(f"R-squared (R2): {r2:.4f}")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape:.4f}")

import numpy as np
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

def calculate_mape(y_true, y_pred):
    """
    A function to calculate Mean Absolute Percentage Error (MAPE)
    that is robust to zero values in y_true.
    """
    # Convert to numpy arrays
    y_true, y_pred = np.array(y_true), np.array(y_pred)

    # Avoid division by zero by replacing true zeros with a small number
    # This prevents the error while having a negligible impact on the overall result.
    mask = y_true != 0

    # Calculate MAPE only for non-zero true values
    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100

    # If all true values are zero, MAPE is undefined. We can return 0 or another indicator.
    if np.all(y_true==0):
        return 0.0

    return mape



# This code assumes you have 'y_test' and 'y_pred_final' from the previous step.

# Standard metrics (no change)
r2 = r2_score(y_test, y_pred_final)
mse = mean_squared_error(y_test, y_pred_final)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred_final)

# Use the new, robust function for MAPE
mape_corrected = calculate_mape(y_test, y_pred_final)

print("\n--- Final Tuned Model Evaluation (Corrected) ---")
print(f"R-squared (R2): {r2:.4f}")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape_corrected:.4f}%") # Corrected MAPE

# --- 5. All Visualizations (Workaround Version) ---
print("\nGenerating visualizations...")

# This version uses fig.show() to display plots directly, avoiding the kaleido error.
# Note: This will not save the Optuna plots as PNG files, but will display them in your environment.

# 5a. Optuna Optimization History
try:
    print("Displaying Optuna optimization history plot...")
    fig = optuna.visualization.plot_optimization_history(study)
    fig.show()
except ImportError:
    print("Could not generate Optuna plots. Please `pip install plotly`.")

# 5b. Optuna Hyperparameter Importance
try:
    print("Displaying Optuna parameter importances plot...")
    fig = optuna.visualization.plot_param_importances(study)
    fig.show()
except ImportError:
     print("Could not generate Optuna plots. Please `pip install plotly`.")


# The following matplotlib plots will still be saved as files as they don't use kaleido.
# 5c. Final Model: Actual vs. Predicted Plot
plt.figure(figsize=(15, 7))
plt.plot(y_test.values, label='Actual Values', color='blue', alpha=0.8)
plt.plot(y_pred_final, label='Predicted Values (Tuned)', color='green', linestyle='--')
plt.title('Final Tuned Model: Predictions vs. Actuals', fontsize=16)
plt.xlabel('Time Points (Test Set)', fontsize=12)
plt.ylabel('Case Count', fontsize=12)
plt.legend()
plt.grid(True)
plt.savefig('final_prediction_vs_actual.png')
print("\nSaved final prediction vs. actual plot to 'final_prediction_vs_actual.png'.")

# 5d. Final Model: Feature Importance Plot
plt.figure(figsize=(12, 15))
xgb.plot_importance(final_tuned_model, max_num_features=len(selected_features_rfe), height=0.8)
plt.title('Final Tuned Model: Feature Importance', fontsize=15)
plt.yticks(fontsize=8)
plt.tight_layout()
plt.savefig('final_feature_importance.png')
print("Saved final feature importance plot to 'final_feature_importance.png'.")

# 5e. Final Model: Tree Visualization

import pandas as pd
import xgboost as xgb
import matplotlib.pyplot as plt
import numpy as np
import optuna
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error
from copy import deepcopy

# --- 5. All Visualizations ---
print("\nGenerating visualizations...")
# Optuna plots require plotly, which may not be installed. Using try-except blocks.
try:
    fig = optuna.visualization.plot_optimization_history(study)
    fig.write_image("optuna_optimization_history.png")
    print("Saved Optuna optimization history plot.")
except ImportError:
    print("Could not generate Optuna plots. Please `pip install plotly`.")

try:
    fig = optuna.visualization.plot_param_importances(study)
    fig.write_image("optuna_param_importances.png")
    print("Saved Optuna parameter importances plot.")
except ImportError:
     print("Could not generate Optuna plots. Please `pip install plotly`.")

plt.figure(figsize=(15, 7))
plt.plot(y_test.values, label='Actual Values', color='blue', alpha=0.8)
plt.plot(y_pred_final, label='Predicted Values (Tuned)', color='green', linestyle='--')
plt.title('Final Tuned Model: Predictions vs. Actuals', fontsize=16)
plt.xlabel('Time Points (Test Set)', fontsize=12)
plt.ylabel('Case Count', fontsize=12)
plt.legend()
plt.grid(True)
plt.savefig('final_prediction_vs_actual.png')
print("Saved final prediction vs. actual plot.")

plt.figure(figsize=(12, 15)) # Adjusted figure size for 87 features
xgb.plot_importance(final_tuned_model, max_num_features=len(selected_features_rfe), height=0.8)
plt.title('Final Tuned Model: Feature Importance', fontsize=15)
plt.yticks(fontsize=8)
plt.tight_layout()
plt.savefig('final_feature_importance.png')
print("Saved final feature importance plot.")
